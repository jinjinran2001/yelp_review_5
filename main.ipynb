{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327d8097-003f-439c-ba03-e0c47e3ec997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internet connection is available.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def check_internet_connection():\n",
    "    try:\n",
    "        # Send a GET request to a reliable website\n",
    "        response = requests.get(\"https://www.google.com\", timeout=5)\n",
    "        \n",
    "        # Check the response status code\n",
    "        if response.status_code == 200:\n",
    "            print(\"Internet connection is available.\")\n",
    "        else:\n",
    "            print(\"Internet connection is not available.\")\n",
    "    \n",
    "    except requests.ConnectionError:\n",
    "        print(\"Internet connection is not available.\")\n",
    "    \n",
    "    except requests.Timeout:\n",
    "        print(\"Request timed out. Internet connection may be slow or unavailable.\")\n",
    "\n",
    "# Call the function to check the internet connection\n",
    "check_internet_connection()                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a62583d5-9e32-4c60-9ebb-0d3689f32fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 03:18:41.802936: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-24 03:18:41.937581: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-24 03:18:43.676322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "from models_1 import GPTModel\n",
    "from helper_function import *\n",
    "from load_gpt2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c513b-c034-4da6-865e-a5c05465aa60",
   "metadata": {},
   "source": [
    "Version Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ecef719-3e44-4dfa-8b35-93102faf7730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.8.3\n",
      "numpy version: 1.26.4\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.3.0+cu118\n",
      "transformers version: 4.39.0.dev0\n",
      "pandas version: 2.2.1\n",
      "tensorflow version: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"transformers\", # For OpenAI's pretrained weights\n",
    "        \"pandas\",      # Dataset loading\n",
    "        \"tensorflow\",\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d186e1fb-e241-4387-a1d6-3db90943621d",
   "metadata": {},
   "source": [
    "Check Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c82699ab-7e24-4b1f-94cf-1c3668029399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 4\n",
      "GPU 0: Tesla V100-SXM2-32GB\n",
      "GPU 1: Tesla V100-SXM2-32GB\n",
      "GPU 2: Tesla V100-SXM2-32GB\n",
      "GPU 3: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print cuda name\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "\n",
    "    # Iterate over the available GPUs and print their names\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i}: {gpu_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2289ecef-7699-4b6f-8b1c-646063b67e7a",
   "metadata": {},
   "source": [
    "Specify tokenizer and padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4d272de-436f-4908-a66f-b897bf52a4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85601d1d-cfd6-4ffe-8daf-e71074d2df84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 995]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode('Hello world', allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce8dc76-c2b3-4bcb-b43e-ffc6376d8792",
   "metadata": {},
   "source": [
    "## Load Pre-trained GPT2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187e57ac-d8ed-42f8-8366-4d62ddf81744",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10c408ae-b8c4-4612-a258-19f36e65bd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8948d990-1891-45ec-a13f-bd06471a02e7",
   "metadata": {},
   "source": [
    "We try the pretrained model without modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7421648-9831-4a92-ae57-01e787e0c33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507db04-d412-447b-91ec-9aa184d2080f",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdb9896f-b3fc-4ed7-85b2-32327fccd664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "train_dataset = RatingDataset(\n",
    "    csv_file=\"data/train_df.csv\",\n",
    "    max_length=300,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4f472dd-28a9-4791-bf7e-a6fb8ce2f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = RatingDataset(\n",
    "    csv_file=\"data/val_df.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = RatingDataset(\n",
    "    csv_file=\"data/test_df.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81a4a73c-b3f8-4c65-9380-bfaac2f35e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 4\n",
    "batch_size = 16\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d839c6c1-bebb-4377-8ee4-cdace0a90faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "300\n",
      "this probably makes me anti-wisconsin... but i simply love this place!  the non-fat chocolate tastes better than the calorie-laden alternatives densely populating the area.  and on a very lucky day, the grapefruit sorbet is the ultimate when fresh from the tap.  hitting that flavor is like winning the lottery!  \\n\\nalso, the guilt-free menu options make it a great date-night stop if you don't want to hear your SO complaining post-indulgence about all of the weight they probably gained.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "tensor(4)\n",
      "Input batch dimensions: torch.Size([16, 300])\n",
      "Label batch dimensions torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    print(len(input_batch[0]))\n",
    "    print(token_ids_to_text(input_batch[0], tokenizer))\n",
    "    print(target_batch[0])\n",
    "    break\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a29e00-7703-48fa-8c32-e1dc8e53a807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32500 training batches\n",
      "8125 validation batches\n",
      "3125 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa597988-0189-448d-82df-c3b756090a79",
   "metadata": {},
   "source": [
    "### Modify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3b7d168-70f7-4089-89b3-f2555dd17811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6931f5-ceac-4d32-b08a-839825a42e48",
   "metadata": {},
   "source": [
    "lock all gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4926675c-b6bf-43d1-a8a5-3f9616a7cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1654e60-add8-484d-be33-9f29f7242df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b34a1af9-b8a6-40d0-9636-05b564a58d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.trf_blocks[-2].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58b23dab-6c52-4975-81a0-785408458c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d63b503-8b51-4541-b48b-2af4a4350789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.3588, -2.0936,  1.8803,  0.0871, -0.3640],\n",
      "         [-7.8132, -8.9034, 11.5499, -4.9334,  2.9435],\n",
      "         [-5.5171, -7.2900,  8.5563, -4.9027,  2.7175],\n",
      "         [-3.3443, -6.5046,  6.4765, -1.9778, -0.4208]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "print(\"Outputs dimensions:\", outputs.shape) # shape: (batch_size, num_tokens, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be1bdde3-5f72-4da1-ae61-1edc4eae25a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.3443, -6.5046,  6.4765, -1.9778, -0.4208]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95866e0a-7e5d-44fc-9891-02fcfeaf6fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 2\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b520614-6292-471f-b1ea-a361f83d0def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 2\n"
     ]
    }
   ],
   "source": [
    "logits = outputs[:, -1, :]\n",
    "label = torch.argmax(logits)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52f6035e-116c-443c-95de-cc258a75c1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/jinran/Review_classfication\n",
      "\u001b[31mERROR: file:///home/jinran/Review_classfication does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce2db6f3-7d6d-4bd7-a22f-d5ac9a8fc65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43303f82-777d-443d-8f65-74decac88b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 18.62%\n",
      "Validation accuracy: 19.75%\n",
      "Test accuracy: 24.62%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42) # For reproducibility due to the shuffling in the training data loader\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=50)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=50)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=50)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\") wquyuiyewuweyuiyuiewqyuijghdaKJCH\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3173280c-9dcf-4cae-9538-171ab44c9a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 6.956\n",
      "Validation loss: 6.781\n",
      "Test loss: 8.158\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
    "\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7545e2a4-ff4f-48f6-902f-ccc28380acb9",
   "metadata": {},
   "source": [
    "# We Train The Model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8781c10-885d-4a95-9cc0-2c226a430c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 0.649, Val loss 0.985\n",
      "Ep 1 (Step 000050): Train loss 0.712, Val loss 0.996\n",
      "Ep 1 (Step 000100): Train loss 0.594, Val loss 0.992\n",
      "Ep 1 (Step 000150): Train loss 0.739, Val loss 0.996\n",
      "Ep 1 (Step 000200): Train loss 0.783, Val loss 1.005\n",
      "Ep 1 (Step 000250): Train loss 0.680, Val loss 0.998\n",
      "Ep 1 (Step 000300): Train loss 0.720, Val loss 1.014\n",
      "Ep 1 (Step 000350): Train loss 0.853, Val loss 1.026\n",
      "Ep 1 (Step 000400): Train loss 0.835, Val loss 0.999\n",
      "Ep 1 (Step 000450): Train loss 0.617, Val loss 1.020\n",
      "Ep 1 (Step 000500): Train loss 0.841, Val loss 1.026\n",
      "Ep 1 (Step 000550): Train loss 0.706, Val loss 1.003\n",
      "Ep 1 (Step 000600): Train loss 0.859, Val loss 1.035\n",
      "Ep 1 (Step 000650): Train loss 0.765, Val loss 1.029\n",
      "Ep 1 (Step 000700): Train loss 0.700, Val loss 1.026\n",
      "Ep 1 (Step 000750): Train loss 0.765, Val loss 1.021\n",
      "Ep 1 (Step 000800): Train loss 0.855, Val loss 1.017\n",
      "Ep 1 (Step 000850): Train loss 0.753, Val loss 0.997\n",
      "Ep 1 (Step 000900): Train loss 0.716, Val loss 0.995\n",
      "Ep 1 (Step 000950): Train loss 0.792, Val loss 1.006\n",
      "Ep 1 (Step 001000): Train loss 0.797, Val loss 1.010\n",
      "Ep 1 (Step 001050): Train loss 0.622, Val loss 1.016\n",
      "Ep 1 (Step 001100): Train loss 0.692, Val loss 1.015\n",
      "Ep 1 (Step 001150): Train loss 0.691, Val loss 1.004\n",
      "Ep 1 (Step 001200): Train loss 0.754, Val loss 1.015\n",
      "Ep 1 (Step 001250): Train loss 0.737, Val loss 1.014\n",
      "Ep 1 (Step 001300): Train loss 0.781, Val loss 0.996\n",
      "Ep 1 (Step 001350): Train loss 0.658, Val loss 1.001\n",
      "Ep 1 (Step 001400): Train loss 0.785, Val loss 0.991\n",
      "Ep 1 (Step 001450): Train loss 0.727, Val loss 0.987\n",
      "Ep 1 (Step 001500): Train loss 0.737, Val loss 0.999\n",
      "Ep 1 (Step 001550): Train loss 0.636, Val loss 1.023\n",
      "Ep 1 (Step 001600): Train loss 0.721, Val loss 1.014\n",
      "Ep 1 (Step 001650): Train loss 0.801, Val loss 1.012\n",
      "Ep 1 (Step 001700): Train loss 0.700, Val loss 1.006\n",
      "Ep 1 (Step 001750): Train loss 0.812, Val loss 1.000\n",
      "Ep 1 (Step 001800): Train loss 0.774, Val loss 1.015\n",
      "Ep 1 (Step 001850): Train loss 0.838, Val loss 1.005\n",
      "Ep 1 (Step 001900): Train loss 0.696, Val loss 1.006\n",
      "Ep 1 (Step 001950): Train loss 0.896, Val loss 1.010\n",
      "Ep 1 (Step 002000): Train loss 0.756, Val loss 1.011\n",
      "Ep 1 (Step 002050): Train loss 0.722, Val loss 0.989\n",
      "Ep 1 (Step 002100): Train loss 0.784, Val loss 1.008\n",
      "Ep 1 (Step 002150): Train loss 0.766, Val loss 1.002\n",
      "Ep 1 (Step 002200): Train loss 0.732, Val loss 1.015\n",
      "Ep 1 (Step 002250): Train loss 0.613, Val loss 1.015\n",
      "Ep 1 (Step 002300): Train loss 0.876, Val loss 1.008\n",
      "Ep 1 (Step 002350): Train loss 0.699, Val loss 1.023\n",
      "Ep 1 (Step 002400): Train loss 0.700, Val loss 1.010\n",
      "Ep 1 (Step 002450): Train loss 0.827, Val loss 0.998\n",
      "Ep 1 (Step 002500): Train loss 0.894, Val loss 1.010\n",
      "Ep 1 (Step 002550): Train loss 0.674, Val loss 1.024\n",
      "Ep 1 (Step 002600): Train loss 0.628, Val loss 1.024\n",
      "Ep 1 (Step 002650): Train loss 0.694, Val loss 1.022\n",
      "Ep 1 (Step 002700): Train loss 0.836, Val loss 1.035\n",
      "Ep 1 (Step 002750): Train loss 0.650, Val loss 0.999\n",
      "Ep 1 (Step 002800): Train loss 0.612, Val loss 1.014\n",
      "Ep 1 (Step 002850): Train loss 0.640, Val loss 1.006\n",
      "Ep 1 (Step 002900): Train loss 0.752, Val loss 0.995\n",
      "Ep 1 (Step 002950): Train loss 0.835, Val loss 0.980\n",
      "Ep 1 (Step 003000): Train loss 0.664, Val loss 0.990\n",
      "Ep 1 (Step 003050): Train loss 0.674, Val loss 0.995\n",
      "Ep 1 (Step 003100): Train loss 0.798, Val loss 0.996\n",
      "Ep 1 (Step 003150): Train loss 0.768, Val loss 0.994\n",
      "Ep 1 (Step 003200): Train loss 0.734, Val loss 0.978\n",
      "Ep 1 (Step 003250): Train loss 0.662, Val loss 0.972\n",
      "Ep 1 (Step 003300): Train loss 0.840, Val loss 0.979\n",
      "Ep 1 (Step 003350): Train loss 0.734, Val loss 0.980\n",
      "Ep 1 (Step 003400): Train loss 0.724, Val loss 0.993\n",
      "Ep 1 (Step 003450): Train loss 0.761, Val loss 0.986\n",
      "Ep 1 (Step 003500): Train loss 0.696, Val loss 1.006\n",
      "Ep 1 (Step 003550): Train loss 0.838, Val loss 1.024\n",
      "Ep 1 (Step 003600): Train loss 0.761, Val loss 0.992\n",
      "Ep 1 (Step 003650): Train loss 0.811, Val loss 0.992\n",
      "Ep 1 (Step 003700): Train loss 0.638, Val loss 1.008\n",
      "Ep 1 (Step 003750): Train loss 0.715, Val loss 0.978\n",
      "Ep 1 (Step 003800): Train loss 0.577, Val loss 0.968\n",
      "Ep 1 (Step 003850): Train loss 0.734, Val loss 0.964\n",
      "Ep 1 (Step 003900): Train loss 0.755, Val loss 0.971\n",
      "Ep 1 (Step 003950): Train loss 0.633, Val loss 0.962\n",
      "Ep 1 (Step 004000): Train loss 0.510, Val loss 0.975\n",
      "Ep 1 (Step 004050): Train loss 0.801, Val loss 0.946\n",
      "Ep 1 (Step 004100): Train loss 0.775, Val loss 0.959\n",
      "Ep 1 (Step 004150): Train loss 1.019, Val loss 0.991\n",
      "Ep 1 (Step 004200): Train loss 0.901, Val loss 0.994\n",
      "Ep 1 (Step 004250): Train loss 0.689, Val loss 1.007\n",
      "Ep 1 (Step 004300): Train loss 0.759, Val loss 0.988\n",
      "Ep 1 (Step 004350): Train loss 0.687, Val loss 0.981\n",
      "Ep 1 (Step 004400): Train loss 0.696, Val loss 0.988\n",
      "Ep 1 (Step 004450): Train loss 0.835, Val loss 0.988\n",
      "Ep 1 (Step 004500): Train loss 0.803, Val loss 0.984\n",
      "Ep 1 (Step 004550): Train loss 0.648, Val loss 0.984\n",
      "Ep 1 (Step 004600): Train loss 0.793, Val loss 0.988\n",
      "Ep 1 (Step 004650): Train loss 0.758, Val loss 1.005\n",
      "Ep 1 (Step 004700): Train loss 0.799, Val loss 1.009\n",
      "Ep 1 (Step 004750): Train loss 0.704, Val loss 0.991\n",
      "Ep 1 (Step 004800): Train loss 0.686, Val loss 0.969\n",
      "Ep 1 (Step 004850): Train loss 0.818, Val loss 0.968\n",
      "Ep 1 (Step 004900): Train loss 0.801, Val loss 0.977\n",
      "Ep 1 (Step 004950): Train loss 0.755, Val loss 0.964\n",
      "Ep 1 (Step 005000): Train loss 0.713, Val loss 0.965\n",
      "Ep 1 (Step 005050): Train loss 0.788, Val loss 0.987\n",
      "Ep 1 (Step 005100): Train loss 0.763, Val loss 0.987\n",
      "Ep 1 (Step 005150): Train loss 0.659, Val loss 1.009\n",
      "Ep 1 (Step 005200): Train loss 0.778, Val loss 0.993\n",
      "Ep 1 (Step 005250): Train loss 0.842, Val loss 1.002\n",
      "Ep 1 (Step 005300): Train loss 0.751, Val loss 0.982\n",
      "Ep 1 (Step 005350): Train loss 0.739, Val loss 0.983\n",
      "Ep 1 (Step 005400): Train loss 0.686, Val loss 0.966\n",
      "Ep 1 (Step 005450): Train loss 0.717, Val loss 0.976\n",
      "Ep 1 (Step 005500): Train loss 0.681, Val loss 0.982\n",
      "Ep 1 (Step 005550): Train loss 0.818, Val loss 0.957\n",
      "Ep 1 (Step 005600): Train loss 0.805, Val loss 0.960\n",
      "Ep 1 (Step 005650): Train loss 0.661, Val loss 0.961\n",
      "Ep 1 (Step 005700): Train loss 0.714, Val loss 0.973\n",
      "Ep 1 (Step 005750): Train loss 0.680, Val loss 0.958\n",
      "Ep 1 (Step 005800): Train loss 0.757, Val loss 0.952\n",
      "Ep 1 (Step 005850): Train loss 0.775, Val loss 0.951\n",
      "Ep 1 (Step 005900): Train loss 0.751, Val loss 0.949\n",
      "Ep 1 (Step 005950): Train loss 0.733, Val loss 0.955\n",
      "Ep 1 (Step 006000): Train loss 0.745, Val loss 0.965\n",
      "Ep 1 (Step 006050): Train loss 0.851, Val loss 0.977\n",
      "Ep 1 (Step 006100): Train loss 0.767, Val loss 0.981\n",
      "Ep 1 (Step 006150): Train loss 0.673, Val loss 0.959\n",
      "Ep 1 (Step 006200): Train loss 0.887, Val loss 0.970\n",
      "Ep 1 (Step 006250): Train loss 0.808, Val loss 0.961\n",
      "Ep 1 (Step 006300): Train loss 0.600, Val loss 0.972\n",
      "Ep 1 (Step 006350): Train loss 0.571, Val loss 0.975\n",
      "Ep 1 (Step 006400): Train loss 0.675, Val loss 0.995\n",
      "Ep 1 (Step 006450): Train loss 0.703, Val loss 1.000\n",
      "Ep 1 (Step 006500): Train loss 0.751, Val loss 1.018\n",
      "Ep 1 (Step 006550): Train loss 0.752, Val loss 1.010\n",
      "Ep 1 (Step 006600): Train loss 0.664, Val loss 0.990\n",
      "Ep 1 (Step 006650): Train loss 0.707, Val loss 0.997\n",
      "Ep 1 (Step 006700): Train loss 0.662, Val loss 1.002\n",
      "Ep 1 (Step 006750): Train loss 0.613, Val loss 1.002\n",
      "Ep 1 (Step 006800): Train loss 0.807, Val loss 0.999\n",
      "Ep 1 (Step 006850): Train loss 0.685, Val loss 0.999\n",
      "Ep 1 (Step 006900): Train loss 0.818, Val loss 0.988\n",
      "Ep 1 (Step 006950): Train loss 0.792, Val loss 0.992\n",
      "Ep 1 (Step 007000): Train loss 0.680, Val loss 0.978\n",
      "Ep 1 (Step 007050): Train loss 0.696, Val loss 0.978\n",
      "Ep 1 (Step 007100): Train loss 0.757, Val loss 0.976\n",
      "Ep 1 (Step 007150): Train loss 0.643, Val loss 0.982\n",
      "Ep 1 (Step 007200): Train loss 0.722, Val loss 0.993\n",
      "Ep 1 (Step 007250): Train loss 0.768, Val loss 0.990\n",
      "Ep 1 (Step 007300): Train loss 0.645, Val loss 1.000\n",
      "Ep 1 (Step 007350): Train loss 0.828, Val loss 0.983\n",
      "Ep 1 (Step 007400): Train loss 0.698, Val loss 0.978\n",
      "Ep 1 (Step 007450): Train loss 0.784, Val loss 1.011\n",
      "Ep 1 (Step 007500): Train loss 0.691, Val loss 0.983\n",
      "Ep 1 (Step 007550): Train loss 0.887, Val loss 1.000\n",
      "Ep 1 (Step 007600): Train loss 0.622, Val loss 0.993\n",
      "Ep 1 (Step 007650): Train loss 0.707, Val loss 0.997\n",
      "Ep 1 (Step 007700): Train loss 0.839, Val loss 1.013\n",
      "Ep 1 (Step 007750): Train loss 0.674, Val loss 0.994\n",
      "Ep 1 (Step 007800): Train loss 0.830, Val loss 0.999\n",
      "Ep 1 (Step 007850): Train loss 0.739, Val loss 0.984\n",
      "Ep 1 (Step 007900): Train loss 0.800, Val loss 0.975\n",
      "Ep 1 (Step 007950): Train loss 0.900, Val loss 0.991\n",
      "Ep 1 (Step 008000): Train loss 0.666, Val loss 0.993\n",
      "Ep 1 (Step 008050): Train loss 0.686, Val loss 0.995\n",
      "Ep 1 (Step 008100): Train loss 0.638, Val loss 0.991\n",
      "Ep 1 (Step 008150): Train loss 0.682, Val loss 0.981\n",
      "Ep 1 (Step 008200): Train loss 0.799, Val loss 0.990\n",
      "Ep 1 (Step 008250): Train loss 0.723, Val loss 0.993\n",
      "Ep 1 (Step 008300): Train loss 0.670, Val loss 0.985\n",
      "Ep 1 (Step 008350): Train loss 0.554, Val loss 0.980\n",
      "Ep 1 (Step 008400): Train loss 0.691, Val loss 0.993\n",
      "Ep 1 (Step 008450): Train loss 0.811, Val loss 1.004\n",
      "Ep 1 (Step 008500): Train loss 0.810, Val loss 0.996\n",
      "Ep 1 (Step 008550): Train loss 0.544, Val loss 0.996\n",
      "Ep 1 (Step 008600): Train loss 0.651, Val loss 0.981\n",
      "Ep 1 (Step 008650): Train loss 0.694, Val loss 0.981\n",
      "Ep 1 (Step 008700): Train loss 0.769, Val loss 0.973\n",
      "Ep 1 (Step 008750): Train loss 0.821, Val loss 0.992\n",
      "Ep 1 (Step 008800): Train loss 0.777, Val loss 0.988\n",
      "Ep 1 (Step 008850): Train loss 0.760, Val loss 0.990\n",
      "Ep 1 (Step 008900): Train loss 0.638, Val loss 0.988\n",
      "Ep 1 (Step 008950): Train loss 0.624, Val loss 0.980\n",
      "Ep 1 (Step 009000): Train loss 0.763, Val loss 0.978\n",
      "Ep 1 (Step 009050): Train loss 0.797, Val loss 0.987\n",
      "Ep 1 (Step 009100): Train loss 0.571, Val loss 0.981\n",
      "Ep 1 (Step 009150): Train loss 0.679, Val loss 0.976\n",
      "Ep 1 (Step 009200): Train loss 0.722, Val loss 0.975\n",
      "Ep 1 (Step 009250): Train loss 0.679, Val loss 1.006\n",
      "Ep 1 (Step 009300): Train loss 0.822, Val loss 0.977\n",
      "Ep 1 (Step 009350): Train loss 0.929, Val loss 0.978\n",
      "Ep 1 (Step 009400): Train loss 0.769, Val loss 0.988\n",
      "Ep 1 (Step 009450): Train loss 0.816, Val loss 0.985\n",
      "Ep 1 (Step 009500): Train loss 0.669, Val loss 0.983\n",
      "Ep 1 (Step 009550): Train loss 0.801, Val loss 0.984\n",
      "Ep 1 (Step 009600): Train loss 0.789, Val loss 1.000\n",
      "Ep 1 (Step 009650): Train loss 0.680, Val loss 0.992\n",
      "Ep 1 (Step 009700): Train loss 0.684, Val loss 0.993\n",
      "Ep 1 (Step 009750): Train loss 0.625, Val loss 0.973\n",
      "Ep 1 (Step 009800): Train loss 0.693, Val loss 0.974\n",
      "Ep 1 (Step 009850): Train loss 0.612, Val loss 0.986\n",
      "Ep 1 (Step 009900): Train loss 0.713, Val loss 0.981\n",
      "Ep 1 (Step 009950): Train loss 0.684, Val loss 0.984\n",
      "Ep 1 (Step 010000): Train loss 0.699, Val loss 0.985\n",
      "Ep 1 (Step 010050): Train loss 0.822, Val loss 0.973\n",
      "Ep 1 (Step 010100): Train loss 0.619, Val loss 0.985\n",
      "Ep 1 (Step 010150): Train loss 0.728, Val loss 0.955\n",
      "Ep 1 (Step 010200): Train loss 0.709, Val loss 0.947\n",
      "Ep 1 (Step 010250): Train loss 0.687, Val loss 0.951\n",
      "Ep 1 (Step 010300): Train loss 0.693, Val loss 0.940\n",
      "Ep 1 (Step 010350): Train loss 0.702, Val loss 0.947\n",
      "Ep 1 (Step 010400): Train loss 0.768, Val loss 0.933\n",
      "Ep 1 (Step 010450): Train loss 0.751, Val loss 0.941\n",
      "Ep 1 (Step 010500): Train loss 0.681, Val loss 0.947\n",
      "Ep 1 (Step 010550): Train loss 0.655, Val loss 0.958\n",
      "Ep 1 (Step 010600): Train loss 0.599, Val loss 0.948\n",
      "Ep 1 (Step 010650): Train loss 0.623, Val loss 0.952\n",
      "Ep 1 (Step 010700): Train loss 0.679, Val loss 0.949\n",
      "Ep 1 (Step 010750): Train loss 0.750, Val loss 0.963\n",
      "Ep 1 (Step 010800): Train loss 0.704, Val loss 0.963\n",
      "Ep 1 (Step 010850): Train loss 0.760, Val loss 0.973\n",
      "Ep 1 (Step 010900): Train loss 0.678, Val loss 0.979\n",
      "Ep 1 (Step 010950): Train loss 0.894, Val loss 0.968\n",
      "Ep 1 (Step 011000): Train loss 0.862, Val loss 0.991\n",
      "Ep 1 (Step 011050): Train loss 0.831, Val loss 0.990\n",
      "Ep 1 (Step 011100): Train loss 0.714, Val loss 0.982\n",
      "Ep 1 (Step 011150): Train loss 0.736, Val loss 0.985\n",
      "Ep 1 (Step 011200): Train loss 0.603, Val loss 0.978\n",
      "Ep 1 (Step 011250): Train loss 0.844, Val loss 0.978\n",
      "Ep 1 (Step 011300): Train loss 0.689, Val loss 0.970\n",
      "Ep 1 (Step 011350): Train loss 0.882, Val loss 0.974\n",
      "Ep 1 (Step 011400): Train loss 0.828, Val loss 0.987\n",
      "Ep 1 (Step 011450): Train loss 0.647, Val loss 0.978\n",
      "Ep 1 (Step 011500): Train loss 0.758, Val loss 0.968\n",
      "Ep 1 (Step 011550): Train loss 0.699, Val loss 0.968\n",
      "Ep 1 (Step 011600): Train loss 0.612, Val loss 0.969\n",
      "Ep 1 (Step 011650): Train loss 0.738, Val loss 0.967\n",
      "Ep 1 (Step 011700): Train loss 0.829, Val loss 0.964\n",
      "Ep 1 (Step 011750): Train loss 0.715, Val loss 0.965\n",
      "Ep 1 (Step 011800): Train loss 0.596, Val loss 0.949\n",
      "Ep 1 (Step 011850): Train loss 0.770, Val loss 0.950\n",
      "Ep 1 (Step 011900): Train loss 0.765, Val loss 0.971\n",
      "Ep 1 (Step 011950): Train loss 0.773, Val loss 0.974\n",
      "Ep 1 (Step 012000): Train loss 0.633, Val loss 0.985\n",
      "Ep 1 (Step 012050): Train loss 0.798, Val loss 0.981\n",
      "Ep 1 (Step 012100): Train loss 0.886, Val loss 1.011\n",
      "Ep 1 (Step 012150): Train loss 0.582, Val loss 0.985\n",
      "Ep 1 (Step 012200): Train loss 0.751, Val loss 0.995\n",
      "Ep 1 (Step 012250): Train loss 0.757, Val loss 0.966\n",
      "Ep 1 (Step 012300): Train loss 0.759, Val loss 0.966\n",
      "Ep 1 (Step 012350): Train loss 0.590, Val loss 0.948\n",
      "Ep 1 (Step 012400): Train loss 0.803, Val loss 0.946\n",
      "Ep 1 (Step 012450): Train loss 0.722, Val loss 0.946\n",
      "Ep 1 (Step 012500): Train loss 0.732, Val loss 0.953\n",
      "Ep 1 (Step 012550): Train loss 0.737, Val loss 0.986\n",
      "Ep 1 (Step 012600): Train loss 0.698, Val loss 0.953\n",
      "Ep 1 (Step 012650): Train loss 0.747, Val loss 0.968\n",
      "Ep 1 (Step 012700): Train loss 0.648, Val loss 0.977\n",
      "Ep 1 (Step 012750): Train loss 0.725, Val loss 0.984\n",
      "Ep 1 (Step 012800): Train loss 0.779, Val loss 0.969\n",
      "Ep 1 (Step 012850): Train loss 0.667, Val loss 0.966\n",
      "Ep 1 (Step 012900): Train loss 0.766, Val loss 0.964\n",
      "Ep 1 (Step 012950): Train loss 0.705, Val loss 0.982\n",
      "Ep 1 (Step 013000): Train loss 0.562, Val loss 1.017\n",
      "Ep 1 (Step 013050): Train loss 0.712, Val loss 0.980\n",
      "Ep 1 (Step 013100): Train loss 0.606, Val loss 0.999\n",
      "Ep 1 (Step 013150): Train loss 0.681, Val loss 1.004\n",
      "Ep 1 (Step 013200): Train loss 0.679, Val loss 1.010\n",
      "Ep 1 (Step 013250): Train loss 0.616, Val loss 1.004\n",
      "Ep 1 (Step 013300): Train loss 0.699, Val loss 0.966\n",
      "Ep 1 (Step 013350): Train loss 0.895, Val loss 0.968\n",
      "Ep 1 (Step 013400): Train loss 0.624, Val loss 0.972\n",
      "Ep 1 (Step 013450): Train loss 0.654, Val loss 0.973\n",
      "Ep 1 (Step 013500): Train loss 0.741, Val loss 0.967\n",
      "Ep 1 (Step 013550): Train loss 0.838, Val loss 0.976\n",
      "Ep 1 (Step 013600): Train loss 0.646, Val loss 0.985\n",
      "Ep 1 (Step 013650): Train loss 0.866, Val loss 0.994\n",
      "Ep 1 (Step 013700): Train loss 0.738, Val loss 1.001\n",
      "Ep 1 (Step 013750): Train loss 0.693, Val loss 1.001\n",
      "Ep 1 (Step 013800): Train loss 0.791, Val loss 1.015\n",
      "Ep 1 (Step 013850): Train loss 0.623, Val loss 1.009\n",
      "Ep 1 (Step 013900): Train loss 0.643, Val loss 1.002\n",
      "Ep 1 (Step 013950): Train loss 0.669, Val loss 0.985\n",
      "Ep 1 (Step 014000): Train loss 0.654, Val loss 1.001\n",
      "Ep 1 (Step 014050): Train loss 0.961, Val loss 0.998\n",
      "Ep 1 (Step 014100): Train loss 0.847, Val loss 1.007\n",
      "Ep 1 (Step 014150): Train loss 0.697, Val loss 0.983\n",
      "Ep 1 (Step 014200): Train loss 0.735, Val loss 0.986\n",
      "Ep 1 (Step 014250): Train loss 0.691, Val loss 0.985\n",
      "Ep 1 (Step 014300): Train loss 0.705, Val loss 0.979\n",
      "Ep 1 (Step 014350): Train loss 0.759, Val loss 0.999\n",
      "Ep 1 (Step 014400): Train loss 0.758, Val loss 0.995\n",
      "Ep 1 (Step 014450): Train loss 0.654, Val loss 0.980\n",
      "Ep 1 (Step 014500): Train loss 0.709, Val loss 0.997\n",
      "Ep 1 (Step 014550): Train loss 0.738, Val loss 0.977\n",
      "Ep 1 (Step 014600): Train loss 0.606, Val loss 0.978\n",
      "Ep 1 (Step 014650): Train loss 0.698, Val loss 0.999\n",
      "Ep 1 (Step 014700): Train loss 0.658, Val loss 1.001\n",
      "Ep 1 (Step 014750): Train loss 0.917, Val loss 0.990\n",
      "Ep 1 (Step 014800): Train loss 0.764, Val loss 0.988\n",
      "Ep 1 (Step 014850): Train loss 0.724, Val loss 1.006\n",
      "Ep 1 (Step 014900): Train loss 0.661, Val loss 1.002\n",
      "Ep 1 (Step 014950): Train loss 0.847, Val loss 0.999\n",
      "Ep 1 (Step 015000): Train loss 0.694, Val loss 0.994\n",
      "Ep 1 (Step 015050): Train loss 0.695, Val loss 0.972\n",
      "Ep 1 (Step 015100): Train loss 0.717, Val loss 0.986\n",
      "Ep 1 (Step 015150): Train loss 0.863, Val loss 0.969\n",
      "Ep 1 (Step 015200): Train loss 0.693, Val loss 0.979\n",
      "Ep 1 (Step 015250): Train loss 0.659, Val loss 0.980\n",
      "Ep 1 (Step 015300): Train loss 0.746, Val loss 0.946\n",
      "Ep 1 (Step 015350): Train loss 0.561, Val loss 0.956\n",
      "Ep 1 (Step 015400): Train loss 0.650, Val loss 0.976\n",
      "Ep 1 (Step 015450): Train loss 0.777, Val loss 0.976\n",
      "Ep 1 (Step 015500): Train loss 0.670, Val loss 0.972\n",
      "Ep 1 (Step 015550): Train loss 0.772, Val loss 0.969\n",
      "Ep 1 (Step 015600): Train loss 0.849, Val loss 0.950\n",
      "Ep 1 (Step 015650): Train loss 0.570, Val loss 0.944\n",
      "Ep 1 (Step 015700): Train loss 0.609, Val loss 0.954\n",
      "Ep 1 (Step 015750): Train loss 0.702, Val loss 0.953\n",
      "Ep 1 (Step 015800): Train loss 0.766, Val loss 0.948\n",
      "Ep 1 (Step 015850): Train loss 0.829, Val loss 0.945\n",
      "Ep 1 (Step 015900): Train loss 0.697, Val loss 0.945\n",
      "Ep 1 (Step 015950): Train loss 0.783, Val loss 0.947\n",
      "Ep 1 (Step 016000): Train loss 0.591, Val loss 0.957\n",
      "Ep 1 (Step 016050): Train loss 0.789, Val loss 0.961\n",
      "Ep 1 (Step 016100): Train loss 0.799, Val loss 0.981\n",
      "Ep 1 (Step 016150): Train loss 0.702, Val loss 0.994\n",
      "Ep 1 (Step 016200): Train loss 0.762, Val loss 0.983\n",
      "Ep 1 (Step 016250): Train loss 1.028, Val loss 0.999\n",
      "Ep 1 (Step 016300): Train loss 0.749, Val loss 0.996\n",
      "Ep 1 (Step 016350): Train loss 0.795, Val loss 1.018\n",
      "Ep 1 (Step 016400): Train loss 0.664, Val loss 0.993\n",
      "Ep 1 (Step 016450): Train loss 0.739, Val loss 0.989\n",
      "Ep 1 (Step 016500): Train loss 0.725, Val loss 0.986\n",
      "Ep 1 (Step 016550): Train loss 0.682, Val loss 0.986\n",
      "Ep 1 (Step 016600): Train loss 0.748, Val loss 0.995\n",
      "Ep 1 (Step 016650): Train loss 0.704, Val loss 0.988\n",
      "Ep 1 (Step 016700): Train loss 0.569, Val loss 1.016\n",
      "Ep 1 (Step 016750): Train loss 0.716, Val loss 1.038\n",
      "Ep 1 (Step 016800): Train loss 0.953, Val loss 1.017\n",
      "Ep 1 (Step 016850): Train loss 0.767, Val loss 1.005\n",
      "Ep 1 (Step 016900): Train loss 0.628, Val loss 0.998\n",
      "Ep 1 (Step 016950): Train loss 0.692, Val loss 0.987\n",
      "Ep 1 (Step 017000): Train loss 0.629, Val loss 1.011\n",
      "Ep 1 (Step 017050): Train loss 0.665, Val loss 1.007\n",
      "Ep 1 (Step 017100): Train loss 0.603, Val loss 1.001\n",
      "Ep 1 (Step 017150): Train loss 0.733, Val loss 1.011\n",
      "Ep 1 (Step 017200): Train loss 0.587, Val loss 1.019\n",
      "Ep 1 (Step 017250): Train loss 0.560, Val loss 1.025\n",
      "Ep 1 (Step 017300): Train loss 0.689, Val loss 1.006\n",
      "Ep 1 (Step 017350): Train loss 0.747, Val loss 1.032\n",
      "Ep 1 (Step 017400): Train loss 0.626, Val loss 0.997\n",
      "Ep 1 (Step 017450): Train loss 0.764, Val loss 1.001\n",
      "Ep 1 (Step 017500): Train loss 0.708, Val loss 0.989\n",
      "Ep 1 (Step 017550): Train loss 0.648, Val loss 0.996\n",
      "Ep 1 (Step 017600): Train loss 0.723, Val loss 0.990\n",
      "Ep 1 (Step 017650): Train loss 0.705, Val loss 1.020\n",
      "Ep 1 (Step 017700): Train loss 0.618, Val loss 1.005\n",
      "Ep 1 (Step 017750): Train loss 0.633, Val loss 0.981\n",
      "Ep 1 (Step 017800): Train loss 0.759, Val loss 0.982\n",
      "Ep 1 (Step 017850): Train loss 0.605, Val loss 0.995\n",
      "Ep 1 (Step 017900): Train loss 0.826, Val loss 0.994\n",
      "Ep 1 (Step 017950): Train loss 0.750, Val loss 1.025\n",
      "Ep 1 (Step 018000): Train loss 0.727, Val loss 0.993\n",
      "Ep 1 (Step 018050): Train loss 0.626, Val loss 0.990\n",
      "Ep 1 (Step 018100): Train loss 0.693, Val loss 0.995\n",
      "Ep 1 (Step 018150): Train loss 0.638, Val loss 0.990\n",
      "Ep 1 (Step 018200): Train loss 0.774, Val loss 1.004\n",
      "Ep 1 (Step 018250): Train loss 0.776, Val loss 1.020\n",
      "Ep 1 (Step 018300): Train loss 0.689, Val loss 1.020\n",
      "Ep 1 (Step 018350): Train loss 0.782, Val loss 1.011\n",
      "Ep 1 (Step 018400): Train loss 0.820, Val loss 1.008\n",
      "Ep 1 (Step 018450): Train loss 0.833, Val loss 1.026\n",
      "Ep 1 (Step 018500): Train loss 0.692, Val loss 1.043\n",
      "Ep 1 (Step 018550): Train loss 0.690, Val loss 1.041\n",
      "Ep 1 (Step 018600): Train loss 0.774, Val loss 1.041\n",
      "Ep 1 (Step 018650): Train loss 0.720, Val loss 1.072\n",
      "Ep 1 (Step 018700): Train loss 0.641, Val loss 1.056\n",
      "Ep 1 (Step 018750): Train loss 0.737, Val loss 1.040\n",
      "Ep 1 (Step 018800): Train loss 0.889, Val loss 1.039\n",
      "Ep 1 (Step 018850): Train loss 0.757, Val loss 1.029\n",
      "Ep 1 (Step 018900): Train loss 0.613, Val loss 1.037\n",
      "Ep 1 (Step 018950): Train loss 0.572, Val loss 1.014\n",
      "Ep 1 (Step 019000): Train loss 0.779, Val loss 1.008\n",
      "Ep 1 (Step 019050): Train loss 0.707, Val loss 1.007\n",
      "Ep 1 (Step 019100): Train loss 0.694, Val loss 1.002\n",
      "Ep 1 (Step 019150): Train loss 0.716, Val loss 0.996\n",
      "Ep 1 (Step 019200): Train loss 0.613, Val loss 1.005\n",
      "Ep 1 (Step 019250): Train loss 0.753, Val loss 1.018\n",
      "Ep 1 (Step 019300): Train loss 0.648, Val loss 1.002\n",
      "Ep 1 (Step 019350): Train loss 0.772, Val loss 1.016\n",
      "Ep 1 (Step 019400): Train loss 0.661, Val loss 1.004\n",
      "Ep 1 (Step 019450): Train loss 0.607, Val loss 0.993\n",
      "Ep 1 (Step 019500): Train loss 0.706, Val loss 1.005\n",
      "Ep 1 (Step 019550): Train loss 0.709, Val loss 1.006\n",
      "Ep 1 (Step 019600): Train loss 0.812, Val loss 1.017\n",
      "Ep 1 (Step 019650): Train loss 0.665, Val loss 1.016\n",
      "Ep 1 (Step 019700): Train loss 0.746, Val loss 1.021\n",
      "Ep 1 (Step 019750): Train loss 0.730, Val loss 1.007\n",
      "Ep 1 (Step 019800): Train loss 0.768, Val loss 1.015\n",
      "Ep 1 (Step 019850): Train loss 0.738, Val loss 1.020\n",
      "Ep 1 (Step 019900): Train loss 0.811, Val loss 1.015\n",
      "Ep 1 (Step 019950): Train loss 0.819, Val loss 1.034\n",
      "Ep 1 (Step 020000): Train loss 0.744, Val loss 1.015\n",
      "Ep 1 (Step 020050): Train loss 0.723, Val loss 1.017\n",
      "Ep 1 (Step 020100): Train loss 0.511, Val loss 1.005\n",
      "Ep 1 (Step 020150): Train loss 0.755, Val loss 1.012\n",
      "Ep 1 (Step 020200): Train loss 0.540, Val loss 1.017\n",
      "Ep 1 (Step 020250): Train loss 0.730, Val loss 1.008\n",
      "Ep 1 (Step 020300): Train loss 0.686, Val loss 1.018\n",
      "Ep 1 (Step 020350): Train loss 0.662, Val loss 0.995\n",
      "Ep 1 (Step 020400): Train loss 0.659, Val loss 1.002\n",
      "Ep 1 (Step 020450): Train loss 0.743, Val loss 1.012\n",
      "Ep 1 (Step 020500): Train loss 0.614, Val loss 1.026\n",
      "Ep 1 (Step 020550): Train loss 0.657, Val loss 0.994\n",
      "Ep 1 (Step 020600): Train loss 0.794, Val loss 0.995\n",
      "Ep 1 (Step 020650): Train loss 0.723, Val loss 1.014\n",
      "Ep 1 (Step 020700): Train loss 0.787, Val loss 1.005\n",
      "Ep 1 (Step 020750): Train loss 0.780, Val loss 1.003\n",
      "Ep 1 (Step 020800): Train loss 0.627, Val loss 1.015\n",
      "Ep 1 (Step 020850): Train loss 0.624, Val loss 0.993\n",
      "Ep 1 (Step 020900): Train loss 0.599, Val loss 1.016\n",
      "Ep 1 (Step 020950): Train loss 0.625, Val loss 1.003\n",
      "Ep 1 (Step 021000): Train loss 0.819, Val loss 1.011\n",
      "Ep 1 (Step 021050): Train loss 0.673, Val loss 0.998\n",
      "Ep 1 (Step 021100): Train loss 0.614, Val loss 1.004\n",
      "Ep 1 (Step 021150): Train loss 0.748, Val loss 1.024\n",
      "Ep 1 (Step 021200): Train loss 0.786, Val loss 1.022\n",
      "Ep 1 (Step 021250): Train loss 0.578, Val loss 1.028\n",
      "Ep 1 (Step 021300): Train loss 0.501, Val loss 1.032\n",
      "Ep 1 (Step 021350): Train loss 0.775, Val loss 1.023\n",
      "Ep 1 (Step 021400): Train loss 0.748, Val loss 1.010\n",
      "Ep 1 (Step 021450): Train loss 0.778, Val loss 0.997\n",
      "Ep 1 (Step 021500): Train loss 0.551, Val loss 0.988\n",
      "Ep 1 (Step 021550): Train loss 0.609, Val loss 0.994\n",
      "Ep 1 (Step 021600): Train loss 0.803, Val loss 0.994\n",
      "Ep 1 (Step 021650): Train loss 0.735, Val loss 0.990\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from train import *\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 1\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0961061f-ea12-45fc-bffd-08cc36e2154a",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ff7bdcf-fc87-479a-93c0-80c70e21c843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 68.25%\n",
      "Validation accuracy: 66.25%\n",
      "Test accuracy: 69.00%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=50)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=50)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=50)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6d3ca1f-f7f6-4585-8b1d-f0aece4143fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "example = \"This is ok\"\n",
    "tokens = text_to_token_ids(example, tokenizer)\n",
    "tokens = tokens.to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(tokens)[:, -1, :]\n",
    "predicted_labels = torch.argmax(logits, dim=-1)\n",
    "print(predicted_labels.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc0abdf2-cff5-45ab-aec1-7cf42f68079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"I hate this film\"\n",
    "def classifier(text):\n",
    "    tokens = text_to_token_ids(text, tokenizer)\n",
    "    tokens = tokens.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(tokens)[:, -1, :]\n",
    "    predicted_labels = torch.argmax(logits, dim=-1)\n",
    "    if predicted_labels.item() == 0:\n",
    "        print('bad')\n",
    "    else:\n",
    "        print('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b28256e6-214d-4add-af48-6d474ea42df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved to:  4_class.pth\n"
     ]
    }
   ],
   "source": [
    "save_model(model, optimizer, '4_class.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323da69-dc66-463a-a2a5-f56d42e505b0",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc13aac2-505f-4a9a-983c-b8796c898f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[7345 2095  447   48   65]\n",
      " [2224 4810 2722  186   58]\n",
      " [ 439 1473 5921 1984  183]\n",
      " [  88  139 2129 5583 2061]\n",
      " [  78   30  376 3076 6440]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have your trained model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Initialize empty lists to store true labels and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Iterate over the test data loader\n",
    "\n",
    "for text, labels in test_loader:\n",
    "    # Move the data to the same device as the model (GPU or CPU)\n",
    "    text = text.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        logits = model(text)[:, -1, :]  # Logits of last output token\n",
    "    predicted = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Append true labels and predicted labels to the lists\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "    predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa9819-1b99-4723-ad45-5878c47f2c46",
   "metadata": {},
   "source": [
    "small model with last two moving gradient results in 0.98"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
